# Описание решений проектной работы

## Задание 1. Планирование архитектуры MongoDB

#### Схема 1: Шардирование (`step1_sharding.drawio`)

**Архитектурные компоненты**:
- **1x pymongo-api**: FastAPI приложение
- **3x mongos роутеры**: `mongos-r1`, `mongos-r2`, `mongos-r3` для балансировки запросов
- **3x Config Servers**: `configSrv-1`, `configSrv-2`, `configSrv-3` для хранения метаданных кластера
- **2x Шарды**: `shard1`, `shard2` для распределения данных

#### Схема 2: Репликация (`step2_replication.drawio`)
**Решаемая проблема**: Обеспечение высокой доступности (High Availability)

**Архитектурные изменения**:
- Каждый шард преобразован в Replica Set с 3 узлами
- Config Servers объединены в Replica Set

#### Схема 3: Кеширование (`step3_caching.drawio`)
**Решаемая проблема**: Максимизация производительности через кеширование горячих данных

**Новые компоненты**:
- **Redis/Valkey Cluster**: расположен справа от приложения для четкого отображения Cache-Aside паттерна
- **Оранжевая толстая стрелка**: приоритетный путь кеширования между app и Redis

### Техническая реализация

**Компоненты схемы 1 (Шардирование)**:
- Приложение подключается ко всем mongos для балансировки
- Роутеры консультируются с Config Servers для получения метаданных
- Данные равномерно распределяются между шардами по hash(user_id)

**Компоненты схемы 2 (Репликация)**:
- Каждый Replica Set содержит 1 Primary + 2 Secondary узла
- Внутренняя репликация обозначена пунктирными цветными линиями
- Запросы идут к Primary узлам, репликация автоматическая

**Компоненты схемы 3 (Кеширование)**:
- Cache-Aside паттерн с приоритетным обращением к Redis/Valkey
- При Cache Miss запрос проходит через всю MongoDB архитектуру
- Результаты кешируются с TTL для последующих запросов

### Результат
Создана полная эволюционная архитектура, готовая к нагрузкам Black Friday:
- **Горизонтальное масштабирование** через шардирование
- **Высокая доступность** через репликацию  
- **Низкая латентность** через кеширование
- **Визуальная ясность** финальных схем для команды разработки
- **Минималистичный дизайн** без информационной перегрузки

---

## Задание 2. Реализация шардирования MongoDB

### Цель
Создать практическую реализацию шардированного кластера MongoDB с автоматической инициализацией и соответствием стандартному workflow разработки.

### Выполненная работа

#### Проект `mongo-sharding`
**Решаемая проблема**: Переход от проектирования к реальной реализации шардированного кластера

**Архитектурные компоненты**:
- **1x Config Server** (`configSrv`) - хранение метаданных кластера (порт 27017)
- **2x Shards** (`shard1`, `shard2`) - распределенное хранение данных (порты 27018, 27019)  
- **1x mongos Router** (`mongos_router`) - маршрутизация запросов (порт 27020)
- **1x FastAPI App** (`pymongo_api`) - веб-приложение (порт 8080)
- **1x Init Container** (`mongo-init`) - автоматическая инициализация Replica Sets

#### Ключевые технические решения

**Автоматическая инициализация**:
- Init-контейнер автоматически инициализирует все Replica Sets при первом запуске
- Правильная последовательность зависимостей через Docker Compose conditions

**Соответствие стандартному workflow**:
```bash
# Запуск всего кластера одной командой
docker compose up -d

# Заполнение тестовыми данными
./scripts/mongo-init.sh
```

**Конфигурация шардирования**:
- Sharding key: `hashed(name)` для равномерного распределения
- Коллекция: `somedb.helloDoc` 
- Тестовые данные: 1000 документов с автоматическим распределением

#### Архитектурные улучшения

**Последовательность запуска**:
1. Базовые сервисы (`configSrv`, `shard1`, `shard2`) → healthy
2. Init-контейнер инициализирует RS → completed successfully  
3. mongos Router подключается к готовым RS → healthy
4. FastAPI приложение подключается к mongos → ready

#### Результаты тестирования

**Успешное распределение данных**:
- Общее количество документов: 1000
- Shard 1: ~500 документов
- Shard 2: ~500 документов
- Равномерное распределение подтверждено

**API функциональность**:
- `GET /` - показывает topology: "Sharded", список шардов
- `GET /helloDoc/count` - общий count через mongos
- `GET /helloDoc/users` - агрегированные данные из всех шардов
- Все эндпоинты работают корректно

### Результат

Создан production-ready шардированный кластер MongoDB:
- **Простота развертывания**: стандартные команды `docker compose up -d`
- **Автоматическая инициализация**: без manual setup steps
- **Горизонтальная масштабируемость**: готовность к добавлению новых шардов
- **Высокая доступность**: Replica Sets для всех критичных компонентов
- **Соответствие best practices**: health checks, restart policies, proper networking

---

## Задание 3. Реализация репликации MongoDB

### Цель
Создать высокодоступную реализацию шардированного кластера MongoDB с полной репликацией для обеспечения отказоустойчивости и автоматического восстановления при сбоях.

### Выполненная работа

#### Проект `mongo-sharding-repl`
**Решаемая проблема**: Переход от базового шардирования к enterprise-ready архитектуре с полной репликацией

**Архитектурные компоненты**:
- **3x Config Server RS** (`configSrv-1,2,3`) - Replica Set для метаданных кластера (порты 27017, 27027, 27037)
- **3x Shard1 RS** (`shard1-1,2,3`) - Replica Set для первого шарда (порты 27018, 27028, 27038)  
- **3x Shard2 RS** (`shard2-1,2,3`) - Replica Set для второго шарда (порты 27019, 27029, 27039)
- **1x mongos Router** (`mongos_router`) - маршрутизация к Replica Sets (порт 27020)
- **1x FastAPI App** (`pymongo_api`) - веб-приложение (порт 8080)
- **1x Init Container** (`mongo-init`) - автоматическая инициализация всех Replica Sets

#### Ключевые технические решения

**Полная репликация**:
- Каждый компонент (Config Servers, Shard1, Shard2) реализован как 3-узловый Replica Set
- 1 Primary + 2 Secondary узла в каждом RS обеспечивают отказоустойчивость
- Автоматическое переключение Primary при сбоях (failover)

**Соответствие enterprise workflow**:
```bash
# Запуск кластера с 9 MongoDB узлами одной командой
docker compose up -d

# Заполнение данными с проверкой репликации
./scripts/mongo-init.sh
```

**Конфигурация высокой доступности**:
- Sharding key: `hashed(name)` для равномерного распределения
- Коллекция: `somedb.helloDoc` 
- Тестовые данные: 1000 документов реплицированы на все узлы
- Выдерживает отказ любого 1 узла из 3 в каждом RS

#### Архитектурные улучшения

**Enterprise-ready конфигурация**:
- 9 MongoDB контейнеров с индивидуальными health checks
- Restart policies (`unless-stopped`) для всех сервисов
- Isolated network с персистентными томами для каждого узла
- Правильные timeout и retry настройки для enterprise нагрузок

**Последовательность запуска**:
1. Все MongoDB узлы (9 контейнеров) → healthy
2. Init-контейнер инициализирует все 3 Replica Sets → completed successfully  
3. mongos Router подключается ко всем Config Servers → healthy
4. FastAPI приложение подключается к mongos → ready

#### Результаты тестирования

**Успешное распределение и репликация данных**:
- Общее количество документов: 1000
- Shard 1: 492 документа (Primary + Secondary узлы)
- Shard 2: 508 документов (Primary + Secondary узлы)
- Данные идентичны на Primary и Secondary узлах каждого RS

**Статус Replica Sets**:
- Config Server RS: configSrv-1 (PRIMARY) + configSrv-2,3 (SECONDARY)
- Shard1 RS: shard1-1 (PRIMARY) + shard1-2,3 (SECONDARY)
- Shard2 RS: shard2-1 (PRIMARY) + shard2-2,3 (SECONDARY)

**API функциональность**:
- `GET /` - показывает topology: "Sharded" с корректными Replica Sets
- `GET /helloDoc/count` - агрегация через mongos из всех шардов  
- `GET /helloDoc/users` - данные из всех реплик работают корректно
- Чтение с Secondary узлов для балансировки нагрузки

### Результат

Создан enterprise-ready высокодоступный шардированный кластер MongoDB:
- **Полная отказоустойчивость**: выдерживает отказ любого узла
- **Автоматическое восстановление**: failover и Primary election
- **Простота развертывания**: стандартные команды `docker compose up -d`
- **Автоматическая инициализация**: без manual setup steps для 9 узлов  
- **Горизонтальная масштабируемость + HA**: готовность к production нагрузкам
- **Соответствие enterprise practices**: health checks, restart policies, proper networking, persistence

---

## Задание 4. Реализация кеширования Redis

### Цель
Создать высокопроизводительную реализацию кеширования для шардированного кластера MongoDB с помощью Redis для обеспечения максимальной производительности под нагрузкой Black Friday.

### Выполненная работа

#### Проект `sharding-repl-cache`
**Решаемая проблема**: Добавление слоя кеширования к enterprise-ready архитектуре для снижения латентности и нагрузки на MongoDB

**Архитектурные компоненты**:
- **9x MongoDB узлов** (Config RS + 2x Shard RS) - полная архитектура из задания 3
- **6x Redis Cluster** (`redis-1` до `redis-6`) - демонстрационный кластер с 3 masters + 3 slaves
- **1x Standalone Redis** (`redis-cache`) - специализированный кеш для приложения
- **1x FastAPI App** (`pymongo_api`) - приложение с включенным кешированием
- **1x Init Container** - автоматическая инициализация MongoDB RS

#### Ключевые технические решения

**Гибридная архитектура Redis**:
- **Redis Cluster** - демонстрирует enterprise подход к распределенному кешированию
- **Standalone Redis** - обеспечивает совместимость с `fastapi-cache2` без изменений кода
- Решает проблему несовместимости Redis Cluster с некоторыми Python клиентами

**Cache-Aside паттерн**:
```bash
# Первый запрос (Cache Miss)
time curl http://localhost:8080/helloDoc/users  # ~1 секунда (MongoDB + запись в кеш)

# Последующие запросы (Cache Hit) 
time curl http://localhost:8080/helloDoc/users  # <100мс (чтение из Redis)
```

**Конфигурация кеширования**:
- TTL: 60 секунд для всех кешируемых эндпоинтов
- Инвалидация: автоматическая при POST/PUT/DELETE операциях
- Кешируемые эндпоинты: `GET /{collection}/users`, `GET /{collection}/users/{name}`

#### Архитектурные улучшения

**Hybrid Redis Solution**:
- Redis Cluster показывает масштабируемое решение для production нагрузок
- Standalone Redis обеспечивает практическую функциональность без сложности кластерного клиента
- Готовность к миграции: в будущем можно заменить клиентскую библиотеку на cluster-aware

#### Результаты тестирования

Статус MongoDB: ![img.png](images/task4/appState.png)

**Производительность кеширования**:
- Cache Miss: `real 0m1.021s` (полная цепочка MongoDB + кеш)
- Cache Hit: `real 0m0.007s` (в 145 раз быстрее!)
- Снижение латентности: с ~1000мс до <10мс
- Достигнута цель <100мс для повторных запросов

**Redis Cluster функциональность**:
- 6-узловой кластер с автоматическим распределением hash slots (16384 слота)
- 3 Master узла + 3 Slave узла для отказоустойчивости
- Успешное тестирование с автоматическими MOVED редиректами
- Кластер готов к горизонтальному масштабированию

**Архитектурная готовность**:
- MongoDB: шардирование + репликация ✅
- Redis: кластеризация + кеширование ✅ 
- Application: Cache-Aside паттерн ✅
- Automation: полностью автоматизированная инициализация ✅

### Результат

Создана финальная enterprise-ready архитектура, готовая к Black Friday нагрузкам:
- **Максимальная производительность**: кеширование снижает латентность в 145 раз
- **Полная отказоустойчивость**: MongoDB репликация + Redis кластер
- **Горизонтальная масштабируемость**: шардирование + распределенный кеш
- **Zero-downtime deployment**: автоматизированная инициализация всех компонентов
- **Соответствие требованиям**: решение без изменений кода приложения

---

### Тестирование кеширования

Первый вызов (Cache Miss):
![cacheMiss.png](images/task4/cacheMiss.png)

Второй вызов (Cache Hit):
![cacheHit.png](images/task4/cacheHit.png)